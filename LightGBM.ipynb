{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5c4958ae-3011-45fb-b642-73099fdd7365",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14543,
    "execution_start": 1643821657066,
    "scrolled": true,
    "source_hash": "927cdba1",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 737
   },
   "source": "!pip install lightgbm\n!pip install imblearn\n!pip install wandb",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: lightgbm in /root/venv/lib/python3.7/site-packages (3.3.2)\nRequirement already satisfied: scikit-learn!=0.22.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from lightgbm) (1.0.1)\nRequirement already satisfied: scipy in /shared-libs/python3.7/py/lib/python3.7/site-packages (from lightgbm) (1.7.3)\nRequirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from lightgbm) (0.37.0)\nRequirement already satisfied: numpy in /shared-libs/python3.7/py/lib/python3.7/site-packages (from lightgbm) (1.19.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.0.0)\nRequirement already satisfied: joblib>=0.11 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: imblearn in /root/venv/lib/python3.7/site-packages (0.0)\nRequirement already satisfied: imbalanced-learn in /root/venv/lib/python3.7/site-packages (from imblearn) (0.9.0)\nRequirement already satisfied: scipy>=1.1.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (3.0.0)\nRequirement already satisfied: joblib>=0.11 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.1.0)\nRequirement already satisfied: numpy>=1.14.6 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.19.5)\nRequirement already satisfied: scikit-learn>=1.0.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.0.1)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\nRequirement already satisfied: wandb in /root/venv/lib/python3.7/site-packages (0.12.10)\nRequirement already satisfied: GitPython>=1.0.0 in /root/venv/lib/python3.7/site-packages (from wandb) (3.1.26)\nRequirement already satisfied: requests<3,>=2.0.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from wandb) (2.26.0)\nRequirement already satisfied: pathtools in /root/venv/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: six>=1.13.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from wandb) (1.16.0)\nRequirement already satisfied: psutil>=5.0.0 in /root/venv/lib/python3.7/site-packages (from wandb) (5.9.0)\nRequirement already satisfied: python-dateutil>=2.6.1 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from wandb) (2.8.2)\nRequirement already satisfied: protobuf>=3.12.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from wandb) (3.19.1)\nRequirement already satisfied: yaspin>=1.0.0 in /root/venv/lib/python3.7/site-packages (from wandb) (2.1.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /root/venv/lib/python3.7/site-packages (from wandb) (1.5.4)\nRequirement already satisfied: PyYAML in /shared-libs/python3.7/py/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: shortuuid>=0.5.0 in /root/venv/lib/python3.7/site-packages (from wandb) (1.0.8)\nRequirement already satisfied: promise<3,>=2.0 in /root/venv/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /root/venv/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from wandb) (8.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /root/venv/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\nRequirement already satisfied: termcolor<2.0.0,>=1.1.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.8.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /root/venv/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->Click!=8.0.0,>=7.0->wandb) (3.6.0)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a090c8de-e990-42d2-a709-39a00e0ae342",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4149,
    "execution_start": 1643821671611,
    "source_hash": "47556b30",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 441
   },
   "source": "import pandas as pd\n# import torch\n# from torch import nn\nimport lightgbm as lgb\nimport datetime\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import KFold\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nimport wandb\nimport pickle\nfrom utils import *",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Load and preprocess data(Feature Engineering using RF.ipynb)",
   "metadata": {
    "cell_id": "335b4025-b941-4384-9968-a8daf9751692",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 134
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8bfb67a5-98d3-43bb-93b6-b73ffd3374f4",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 589,
    "execution_start": 1643821675761,
    "source_hash": "c0a4d37a",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 631.59375
   },
   "source": "data_df=load_processed_csv(train=True)\ndata_df",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 3,
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 73,
       "row_count": 9557,
       "columns": [
        {
         "name": "feature0",
         "dtype": "float64",
         "stats": {
          "unique_count": 166,
          "nan_count": 0,
          "min": "-1.5167201507306425",
          "max": "21.014290888221428",
          "histogram": [
           {
            "bin_start": -1.5167201507306425,
            "bin_end": 0.7363809531645646,
            "count": 8086
           },
           {
            "bin_start": 0.7363809531645646,
            "bin_end": 2.9894820570597718,
            "count": 1340
           },
           {
            "bin_start": 2.9894820570597718,
            "bin_end": 5.242583160954979,
            "count": 104
           },
           {
            "bin_start": 5.242583160954979,
            "bin_end": 7.495684264850186,
            "count": 18
           },
           {
            "bin_start": 7.495684264850186,
            "bin_end": 9.748785368745395,
            "count": 7
           },
           {
            "bin_start": 9.748785368745395,
            "bin_end": 12.0018864726406,
            "count": 0
           },
           {
            "bin_start": 12.0018864726406,
            "bin_end": 14.254987576535807,
            "count": 0
           },
           {
            "bin_start": 14.254987576535807,
            "bin_end": 16.508088680431015,
            "count": 0
           },
           {
            "bin_start": 16.508088680431015,
            "bin_end": 18.761189784326223,
            "count": 0
           },
           {
            "bin_start": 18.761189784326223,
            "bin_end": 21.014290888221428,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "feature1",
         "dtype": "float64",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 9193
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 0
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 0
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 0
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 0
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 0
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 0
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 364
           }
          ]
         }
        },
        {
         "name": "feature2",
         "dtype": "float64",
         "stats": {
          "unique_count": 11,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 97
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 188
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 3477
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 0
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 2940
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 1607
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 732
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 298
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 168
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 50
           }
          ]
         }
        },
        {
         "name": "feature3",
         "dtype": "float64",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 9331
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 0
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 0
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 0
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 0
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 0
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 0
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 226
           }
          ]
         }
        },
        {
         "name": "feature4",
         "dtype": "float64",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 50
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 0
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 0
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 0
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 0
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 0
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 0
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 9507
           }
          ]
         }
        },
        {
         "name": "feature5",
         "dtype": "float64",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 405
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 0
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 0
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 0
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 0
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 0
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 0
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 9152
           }
          ]
         }
        },
        {
         "name": "feature6",
         "dtype": "float64",
         "stats": {
          "unique_count": 2,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 7342
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 0
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 0
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 0
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 0
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 0
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 0
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 2215
           }
          ]
         }
        },
        {
         "name": "feature7",
         "dtype": "float64",
         "stats": {
          "unique_count": 9,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0000000000000002",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.10000000000000002,
            "count": 7186
           },
           {
            "bin_start": 0.10000000000000002,
            "bin_end": 0.20000000000000004,
            "count": 1125
           },
           {
            "bin_start": 0.20000000000000004,
            "bin_end": 0.30000000000000004,
            "count": 3
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.4000000000000001,
            "bin_end": 0.5000000000000001,
            "count": 1070
           },
           {
            "bin_start": 0.5000000000000001,
            "bin_end": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000002,
            "count": 135
           },
           {
            "bin_start": 0.7000000000000002,
            "bin_end": 0.8000000000000002,
            "count": 32
           },
           {
            "bin_start": 0.8000000000000002,
            "bin_end": 0.9000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.9000000000000001,
            "bin_end": 1.0000000000000002,
            "count": 6
           }
          ]
         }
        },
        {
         "name": "feature8",
         "dtype": "float64",
         "stats": {
          "unique_count": 6,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 6718
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 0
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 2141
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 0
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 607
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 53
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 0
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 16
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 22
           }
          ]
         }
        },
        {
         "name": "feature9",
         "dtype": "float64",
         "stats": {
          "unique_count": 9,
          "nan_count": 0,
          "min": "0.0",
          "max": "1.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 954
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 4370
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 2775
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 1057
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 0
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 278
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 71
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 31
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 11
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 10
           }
          ]
         }
        },
        {
         "name": "feature10",
         "dtype": "float64"
        },
        {
         "name": "feature11",
         "dtype": "float64"
        },
        {
         "name": "feature12",
         "dtype": "float64"
        },
        {
         "name": "feature13",
         "dtype": "float64"
        },
        {
         "name": "feature14",
         "dtype": "float64"
        },
        {
         "name": "feature15",
         "dtype": "float64"
        },
        {
         "name": "feature16",
         "dtype": "float64"
        },
        {
         "name": "feature17",
         "dtype": "float64"
        },
        {
         "name": "feature18",
         "dtype": "float64"
        },
        {
         "name": "feature19",
         "dtype": "float64"
        },
        {
         "name": "feature20",
         "dtype": "float64"
        },
        {
         "name": "feature21",
         "dtype": "float64"
        },
        {
         "name": "feature22",
         "dtype": "float64"
        },
        {
         "name": "feature23",
         "dtype": "float64"
        },
        {
         "name": "feature24",
         "dtype": "float64"
        },
        {
         "name": "feature25",
         "dtype": "float64"
        },
        {
         "name": "feature26",
         "dtype": "float64"
        },
        {
         "name": "feature27",
         "dtype": "float64"
        },
        {
         "name": "feature28",
         "dtype": "float64"
        },
        {
         "name": "feature29",
         "dtype": "float64"
        },
        {
         "name": "feature30",
         "dtype": "float64"
        },
        {
         "name": "feature31",
         "dtype": "float64"
        },
        {
         "name": "feature32",
         "dtype": "float64"
        },
        {
         "name": "feature33",
         "dtype": "float64"
        },
        {
         "name": "feature34",
         "dtype": "float64"
        },
        {
         "name": "feature35",
         "dtype": "float64"
        },
        {
         "name": "feature36",
         "dtype": "float64"
        },
        {
         "name": "feature37",
         "dtype": "float64"
        },
        {
         "name": "feature38",
         "dtype": "float64"
        },
        {
         "name": "feature39",
         "dtype": "float64"
        },
        {
         "name": "feature40",
         "dtype": "float64"
        },
        {
         "name": "feature41",
         "dtype": "float64"
        },
        {
         "name": "feature42",
         "dtype": "float64"
        },
        {
         "name": "feature43",
         "dtype": "float64"
        },
        {
         "name": "feature44",
         "dtype": "float64"
        },
        {
         "name": "feature45",
         "dtype": "float64"
        },
        {
         "name": "feature46",
         "dtype": "float64"
        },
        {
         "name": "feature47",
         "dtype": "float64"
        },
        {
         "name": "feature48",
         "dtype": "float64"
        },
        {
         "name": "feature49",
         "dtype": "float64"
        },
        {
         "name": "feature50",
         "dtype": "float64"
        },
        {
         "name": "feature51",
         "dtype": "float64"
        },
        {
         "name": "feature52",
         "dtype": "float64"
        },
        {
         "name": "feature53",
         "dtype": "float64"
        },
        {
         "name": "feature54",
         "dtype": "float64"
        },
        {
         "name": "feature55",
         "dtype": "float64"
        },
        {
         "name": "feature56",
         "dtype": "float64"
        },
        {
         "name": "feature57",
         "dtype": "float64"
        },
        {
         "name": "feature58",
         "dtype": "int64"
        },
        {
         "name": "feature59",
         "dtype": "int64"
        },
        {
         "name": "feature60",
         "dtype": "int64"
        },
        {
         "name": "feature61",
         "dtype": "int64"
        },
        {
         "name": "feature62",
         "dtype": "int64"
        },
        {
         "name": "feature63",
         "dtype": "int64"
        },
        {
         "name": "feature64",
         "dtype": "int64"
        },
        {
         "name": "feature65",
         "dtype": "int64"
        },
        {
         "name": "feature66",
         "dtype": "int64"
        },
        {
         "name": "feature67",
         "dtype": "int64"
        },
        {
         "name": "feature68",
         "dtype": "int64"
        },
        {
         "name": "feature69",
         "dtype": "int64"
        },
        {
         "name": "feature70",
         "dtype": "int64"
        },
        {
         "name": "feature71",
         "dtype": "int64"
        },
        {
         "name": "label",
         "dtype": "int64"
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows": [
        {
         "feature0": 0.3022447668788747,
         "feature1": 0,
         "feature2": 0.2,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 0,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.125,
         "feature10": 0.125,
         "feature11": 0,
         "feature12": 0,
         "feature13": 0,
         "feature14": 0,
         "feature15": 0,
         "feature16": 0,
         "feature17": 0.4761904761904761,
         "feature18": 0,
         "feature19": 0,
         "feature20": 0,
         "feature21": 1,
         "feature22": 0,
         "feature23": 0,
         "feature24": 0,
         "feature25": 0,
         "feature26": 1,
         "feature27": 0,
         "feature28": 0,
         "feature29": 0,
         "feature30": 1,
         "feature31": 0,
         "feature32": 0,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0,
         "feature43": 0.1111111111111111,
         "feature44": 0,
         "feature45": 0,
         "feature46": 10,
         "feature47": 0,
         "feature48": 0.2702702702702703,
         "feature49": 0,
         "feature50": 0.1379310344827586,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.1,
         "feature56": 1,
         "feature57": 0.4432989690721649,
         "feature58": 1,
         "feature59": 0,
         "feature60": 0,
         "feature61": 3,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 2,
         "feature66": 1,
         "feature67": 0,
         "feature68": 0,
         "feature69": 1,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 0
        },
        {
         "feature0": -0.2242977092712486,
         "feature1": 0,
         "feature2": 0.3,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 1,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.125,
         "feature10": 0.125,
         "feature11": 0,
         "feature12": 0,
         "feature13": 0,
         "feature14": 0,
         "feature15": 0,
         "feature16": 0,
         "feature17": 0.5714285714285714,
         "feature18": 0.1,
         "feature19": 0,
         "feature20": 0,
         "feature21": 1,
         "feature22": 0,
         "feature23": 0,
         "feature24": 0,
         "feature25": 0,
         "feature26": 1,
         "feature27": 0,
         "feature28": 0,
         "feature29": 0,
         "feature30": 1,
         "feature31": 0,
         "feature32": 0,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0,
         "feature43": 0.1111111111111111,
         "feature44": 0.3333333333333333,
         "feature45": 8,
         "feature46": 12,
         "feature47": 0,
         "feature48": 0.3243243243243243,
         "feature49": 0,
         "feature50": 0.1379310344827586,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.1,
         "feature56": 1,
         "feature57": 0.6907216494845361,
         "feature58": 1,
         "feature59": 1,
         "feature60": 1,
         "feature61": 7,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 1,
         "feature66": 1,
         "feature67": 4,
         "feature68": 5,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 1
        },
        {
         "feature0": 0.2065097712152159,
         "feature1": 0,
         "feature2": 0.7000000000000001,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 0,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0,
         "feature10": 0,
         "feature11": 0,
         "feature12": 0.1666666666666666,
         "feature13": 0.125,
         "feature14": 0,
         "feature15": 0,
         "feature16": 0,
         "feature17": 0.5238095238095237,
         "feature18": 0.1,
         "feature19": 0,
         "feature20": 1,
         "feature21": 0,
         "feature22": 1,
         "feature23": 0,
         "feature24": 0,
         "feature25": 0,
         "feature26": 0,
         "feature27": 0,
         "feature28": 1,
         "feature29": 0,
         "feature30": 1,
         "feature31": 0,
         "feature32": 0,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0,
         "feature43": 0.1111111111111111,
         "feature44": 0.3333333333333333,
         "feature45": 8,
         "feature46": 0,
         "feature47": 11,
         "feature48": 0.2972972972972973,
         "feature49": 0.1428571428571428,
         "feature50": 0.0517241379310344,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 0,
         "feature55": 0,
         "feature56": 1,
         "feature57": 0.9484536082474226,
         "feature58": 1,
         "feature59": 2,
         "feature60": 2,
         "feature61": 4,
         "feature62": 0,
         "feature63": 0,
         "feature64": 0,
         "feature65": 1,
         "feature66": 1,
         "feature67": 4,
         "feature68": 0,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 2
        },
        {
         "feature0": 0.2065097712152159,
         "feature1": 0,
         "feature2": 0.4,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 1,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.25,
         "feature10": 0.25,
         "feature11": 0.1666666666666666,
         "feature12": 0.1666666666666666,
         "feature13": 0.25,
         "feature14": 0.1428571428571428,
         "feature15": 0.2,
         "feature16": 0.2142857142857142,
         "feature17": 0.4285714285714285,
         "feature18": 0.2,
         "feature19": 0.25,
         "feature20": 0,
         "feature21": 1,
         "feature22": 0,
         "feature23": 0,
         "feature24": 0,
         "feature25": 0,
         "feature26": 0,
         "feature27": 0,
         "feature28": 0,
         "feature29": 1,
         "feature30": 0,
         "feature31": 0,
         "feature32": 1,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0.2222222222222222,
         "feature43": 0.2222222222222222,
         "feature44": 0,
         "feature45": 1,
         "feature46": 11,
         "feature47": 0,
         "feature48": 0.2972972972972973,
         "feature49": 0.2857142857142857,
         "feature50": 0.1954023103448275,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.3,
         "feature56": 1,
         "feature57": 0.1752577319587628,
         "feature58": 2,
         "feature59": 2,
         "feature60": 2,
         "feature61": 3,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 1,
         "feature66": 1,
         "feature67": 0,
         "feature68": 0,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 3
        },
        {
         "feature0": 0.2065097712152159,
         "feature1": 0,
         "feature2": 0.4,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 1,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.25,
         "feature10": 0.25,
         "feature11": 0.1666666666666666,
         "feature12": 0.1666666666666666,
         "feature13": 0.25,
         "feature14": 0.1428571428571428,
         "feature15": 0.2,
         "feature16": 0.2142857142857142,
         "feature17": 0.5238095238095237,
         "feature18": 0.1,
         "feature19": 0.25,
         "feature20": 0,
         "feature21": 0,
         "feature22": 1,
         "feature23": 0,
         "feature24": 1,
         "feature25": 0,
         "feature26": 0,
         "feature27": 0,
         "feature28": 0,
         "feature29": 0,
         "feature30": 0,
         "feature31": 1,
         "feature32": 0,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0.2222222222222222,
         "feature43": 0.2222222222222222,
         "feature44": 0,
         "feature45": 1,
         "feature46": 11,
         "feature47": 0,
         "feature48": 0.2972972972972973,
         "feature49": 0.2857142857142857,
         "feature50": 0.1954023103448275,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.3,
         "feature56": 1,
         "feature57": 0.3814432989690721,
         "feature58": 2,
         "feature59": 2,
         "feature60": 2,
         "feature61": 4,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 1,
         "feature66": 1,
         "feature67": 0,
         "feature68": 0,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 4
        },
        {
         "feature0": 0.2065097712152159,
         "feature1": 0,
         "feature2": 0.4,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 1,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.25,
         "feature10": 0.25,
         "feature11": 0.1666666666666666,
         "feature12": 0.1666666666666666,
         "feature13": 0.25,
         "feature14": 0.1428571428571428,
         "feature15": 0.2,
         "feature16": 0.2142857142857142,
         "feature17": 0.5238095238095237,
         "feature18": 0.1,
         "feature19": 0.25,
         "feature20": 0,
         "feature21": 1,
         "feature22": 0,
         "feature23": 0,
         "feature24": 1,
         "feature25": 0,
         "feature26": 0,
         "feature27": 0,
         "feature28": 0,
         "feature29": 0,
         "feature30": 1,
         "feature31": 0,
         "feature32": 0,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0.2222222222222222,
         "feature43": 0.2222222222222222,
         "feature44": 0,
         "feature45": 1,
         "feature46": 11,
         "feature47": 0,
         "feature48": 0.2972972972972973,
         "feature49": 0.2857142857142857,
         "feature50": 0.1954023103448275,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.3,
         "feature56": 1,
         "feature57": 0.3917525773195876,
         "feature58": 2,
         "feature59": 2,
         "feature60": 2,
         "feature61": 4,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 1,
         "feature66": 1,
         "feature67": 0,
         "feature68": 0,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 5
        },
        {
         "feature0": 0.2065097712152159,
         "feature1": 0,
         "feature2": 0.4,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 1,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.25,
         "feature10": 0.25,
         "feature11": 0.1666666666666666,
         "feature12": 0.1666666666666666,
         "feature13": 0.25,
         "feature14": 0.1428571428571428,
         "feature15": 0.2,
         "feature16": 0.2142857142857142,
         "feature17": 0.0952380952380952,
         "feature18": 0,
         "feature19": 0.25,
         "feature20": 0,
         "feature21": 0,
         "feature22": 1,
         "feature23": 1,
         "feature24": 0,
         "feature25": 0,
         "feature26": 0,
         "feature27": 0,
         "feature28": 0,
         "feature29": 0,
         "feature30": 0,
         "feature31": 0,
         "feature32": 1,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0.2222222222222222,
         "feature43": 0.2222222222222222,
         "feature44": 0,
         "feature45": 1,
         "feature46": 11,
         "feature47": 0,
         "feature48": 0.2972972972972973,
         "feature49": 0.2857142857142857,
         "feature50": 0.1954023103448275,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.3,
         "feature56": 1,
         "feature57": 0.0824742268041237,
         "feature58": 2,
         "feature59": 2,
         "feature60": 2,
         "feature61": 1,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 1,
         "feature66": 1,
         "feature67": 0,
         "feature68": 0,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 6
        },
        {
         "feature0": -0.272165207103078,
         "feature1": 1,
         "feature2": 0.1,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 0,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.125,
         "feature10": 0.125,
         "feature11": 0.3333333333333333,
         "feature12": 0.1666666666666666,
         "feature13": 0.375,
         "feature14": 0.2857142857142857,
         "feature15": 0.1,
         "feature16": 0.2142857142857142,
         "feature17": 0,
         "feature18": 0,
         "feature19": 0.25,
         "feature20": 0,
         "feature21": 0,
         "feature22": 1,
         "feature23": 1,
         "feature24": 0,
         "feature25": 0,
         "feature26": 0,
         "feature27": 0,
         "feature28": 0,
         "feature29": 0,
         "feature30": 0,
         "feature31": 0,
         "feature32": 1,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0.2222222222222222,
         "feature43": 0.2222222222222222,
         "feature44": 0,
         "feature45": 1,
         "feature46": 9,
         "feature47": 0,
         "feature48": 0.2702702702702703,
         "feature49": 0,
         "feature50": 0.6551724137931035,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.1,
         "feature56": 1,
         "feature57": 0.0721649484536082,
         "feature58": 0,
         "feature59": 0,
         "feature60": 1,
         "feature61": 0,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 2,
         "feature66": 1,
         "feature67": 0,
         "feature68": 0,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 7
        },
        {
         "feature0": -0.272165207103078,
         "feature1": 1,
         "feature2": 0.1,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 0,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.125,
         "feature10": 0.125,
         "feature11": 0.3333333333333333,
         "feature12": 0.1666666666666666,
         "feature13": 0.375,
         "feature14": 0.2857142857142857,
         "feature15": 0.1,
         "feature16": 0.2142857142857142,
         "feature17": 0.4285714285714285,
         "feature18": 0.1,
         "feature19": 0.25,
         "feature20": 0,
         "feature21": 1,
         "feature22": 0,
         "feature23": 0,
         "feature24": 1,
         "feature25": 0,
         "feature26": 0,
         "feature27": 0,
         "feature28": 0,
         "feature29": 0,
         "feature30": 1,
         "feature31": 0,
         "feature32": 0,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0.2222222222222222,
         "feature43": 0.2222222222222222,
         "feature44": 0,
         "feature45": 1,
         "feature46": 9,
         "feature47": 0,
         "feature48": 0.2702702702702703,
         "feature49": 0,
         "feature50": 0.6551724137931035,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.1,
         "feature56": 1,
         "feature57": 0.3092783505154639,
         "feature58": 0,
         "feature59": 0,
         "feature60": 1,
         "feature61": 3,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 2,
         "feature66": 1,
         "feature67": 0,
         "feature68": 0,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 8
        },
        {
         "feature0": -0.272165207103078,
         "feature1": 1,
         "feature2": 0.1,
         "feature3": 0,
         "feature4": 1,
         "feature5": 1,
         "feature6": 0,
         "feature7": 0,
         "feature8": 0,
         "feature9": 0.125,
         "feature10": 0.125,
         "feature11": 0.3333333333333333,
         "feature12": 0.1666666666666666,
         "feature13": 0.375,
         "feature14": 0.2857142857142857,
         "feature15": 0.1,
         "feature16": 0.2142857142857142,
         "feature17": 0.5238095238095237,
         "feature18": 0.1,
         "feature19": 0.25,
         "feature20": 0,
         "feature21": 0,
         "feature22": 1,
         "feature23": 0,
         "feature24": 1,
         "feature25": 0,
         "feature26": 0,
         "feature27": 0,
         "feature28": 0,
         "feature29": 0,
         "feature30": 0,
         "feature31": 1,
         "feature32": 0,
         "feature33": 0,
         "feature34": 0,
         "feature35": 0,
         "feature36": 0,
         "feature37": 0,
         "feature38": 0,
         "feature39": 0,
         "feature40": 0,
         "feature41": 0,
         "feature42": 0.2222222222222222,
         "feature43": 0.2222222222222222,
         "feature44": 0,
         "feature45": 1,
         "feature46": 9,
         "feature47": 0,
         "feature48": 0.2702702702702703,
         "feature49": 0,
         "feature50": 0.6551724137931035,
         "feature51": 0,
         "feature52": 0,
         "feature53": 0,
         "feature54": 1,
         "feature55": 0.1,
         "feature56": 1,
         "feature57": 0.2886597938144329,
         "feature58": 0,
         "feature59": 0,
         "feature60": 1,
         "feature61": 4,
         "feature62": 2,
         "feature63": 0,
         "feature64": 0,
         "feature65": 2,
         "feature66": 1,
         "feature67": 0,
         "feature68": 0,
         "feature69": 0,
         "feature70": 0,
         "feature71": 0,
         "label": 3,
         "_deepnote_index_column": 9
        }
       ]
      },
      "text/plain": "      feature0  feature1  feature2  feature3  feature4  feature5  feature6  \\\n0     0.302245       0.0       0.2       0.0       1.0       1.0       0.0   \n1    -0.224298       0.0       0.3       0.0       1.0       1.0       1.0   \n2     0.206510       0.0       0.7       0.0       1.0       1.0       0.0   \n3     0.206510       0.0       0.4       0.0       1.0       1.0       1.0   \n4     0.206510       0.0       0.4       0.0       1.0       1.0       1.0   \n...        ...       ...       ...       ...       ...       ...       ...   \n9552 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n9553 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n9554 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n9555 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n9556 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n\n      feature7  feature8  feature9  ...  feature63  feature64  feature65  \\\n0          0.0       0.0     0.125  ...          0          0          2   \n1          0.0       0.0     0.125  ...          0          0          1   \n2          0.0       0.0     0.000  ...          0          0          1   \n3          0.0       0.0     0.250  ...          0          0          1   \n4          0.0       0.0     0.250  ...          0          0          1   \n...        ...       ...       ...  ...        ...        ...        ...   \n9552       0.0       0.0     0.250  ...          5          1          2   \n9553       0.0       0.0     0.250  ...          5          1          2   \n9554       0.0       0.0     0.250  ...          5          1          2   \n9555       0.0       0.0     0.250  ...          5          1          2   \n9556       0.0       0.0     0.250  ...          5          1          2   \n\n      feature66  feature67  feature68  feature69  feature70  feature71  label  \n0             1          0          0          1          0          0      3  \n1             1          4          5          0          0          0      3  \n2             1          4          0          0          0          0      3  \n3             1          0          0          0          0          0      3  \n4             1          0          0          0          0          0      3  \n...         ...        ...        ...        ...        ...        ...    ...  \n9552          2          0          1          0          0          3      1  \n9553          2          0          1          0          0          3      1  \n9554          2          0          1          0          0          3      1  \n9555          2          0          1          0          0          3      1  \n9556          2          0          1          0          0          3      1  \n\n[9557 rows x 73 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature0</th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>feature4</th>\n      <th>feature5</th>\n      <th>feature6</th>\n      <th>feature7</th>\n      <th>feature8</th>\n      <th>feature9</th>\n      <th>...</th>\n      <th>feature63</th>\n      <th>feature64</th>\n      <th>feature65</th>\n      <th>feature66</th>\n      <th>feature67</th>\n      <th>feature68</th>\n      <th>feature69</th>\n      <th>feature70</th>\n      <th>feature71</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.302245</td>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.125</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.224298</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.125</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.206510</td>\n      <td>0.0</td>\n      <td>0.7</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.206510</td>\n      <td>0.0</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.206510</td>\n      <td>0.0</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9552</th>\n      <td>-0.750840</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9553</th>\n      <td>-0.750840</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9554</th>\n      <td>-0.750840</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9555</th>\n      <td>-0.750840</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9556</th>\n      <td>-0.750840</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250</td>\n      <td>...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>9557 rows × 73 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "744be497-8960-4c39-aec6-af4b687374d2",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1643821676336,
    "source_hash": "29a14886",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "# plt.figure(figsize=(50,50))\n# sb.heatmap(data_df.corr(), xticklabels=data_df.corr().columns, yticklabels=data_df.corr().columns, cmap='RdYlGn', center=0, annot=True)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Define KFolder and light_bgm Function",
   "metadata": {
    "cell_id": "2aa3e205-34c0-4b5b-865a-057ac91b3664",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 82
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "43b35178-3754-48c7-8c2c-9b01ba8dbdbb",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1643821676337,
    "source_hash": "db929e46",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 711
   },
   "source": "def light_bgm_train(X_train,y_train,X_test,y_test,params=None):\n    train_data=lgb.Dataset(X_train,label=y_train)\n    validation_data=lgb.Dataset(X_test,label=y_test)\n    if params is None:\n        params={\n            'learning_rate':0.1,\n            'lambda_l1':0.1,\n            'lambda_l2':0.2,\n            'max_depth':10,#default 6\n            'objective':'multiclass',\n            'num_class':4,  \n            # 'max_bin':255,# 大会有更准的效果,更慢的速度\n            # 'max_depth': -1,  # 小数据集下限制最大深度可防止过拟合,小于0表示无限制\n            # 'num_leaves': 64,  # 大会更准,但可能过拟合\n            # 'feature_fraction': 0.8,  # 防止过拟合\n            # 'bagging_freq': 5,  # 防止过拟合\n            # 'bagging_fraction': 0.8,  # 防止过拟合\n            # 'min_data_in_leaf': 21,  # 防止过拟合\n            # 'min_sum_hessian_in_leaf': 3.0,  # 防止过拟合\n        }\n    clf=lgb.train(params,train_data,valid_sets=[validation_data])\n    return clf\n\ndef k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n    X,Y=np.array(X),np.array(Y)\n    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n    for train_index, test_index in kf.split(X):\n        X_train = X[train_index]\n        y_train = Y[train_index]\n        X_test = X[test_index]\n        y_test = Y[test_index]\n        model = light_bgm_train(X_train,y_train,X_test,y_test,params)\n        yield (model,test_index)\n# X = data_df.iloc[:, :-1]\n# Y = data_df[\"label\"]\n# k_fold(3,X,Y)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Confusion Matrix and Report",
   "metadata": {
    "cell_id": "0c91a606-f40e-4a21-b80f-a40eb769657d",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 82
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a80e590b-3484-415a-bff9-88ea8d5a2faf",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 204,
    "execution_start": 1643821676384,
    "source_hash": "b0ec0193",
    "tags": [],
    "owner_user_id": "1e899dee-29f9-4275-a6ea-e33c24f3b20c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 258
   },
   "source": "X = data_df.iloc[:, :-1]\nY = data_df[\"label\"]\nprint(\"original distribution:\",Counter(Y))\n#upsamlping \nsmo = SMOTE(random_state=42)\nX_smo, Y_smo = smo.fit_resample(X, Y)\nprint(\"smote distribution:\",Counter(Y_smo))\nX,Y=X_smo, Y_smo#using smote dataset",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "original distribution: Counter({3: 5996, 1: 1597, 2: 1209, 0: 755})\nsmote distribution: Counter({3: 5996, 1: 5996, 2: 5996, 0: 5996})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00009-0a4e7307-f55b-4faa-8f4c-90c9edbb1ec4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "82f2cfbb",
    "execution_start": 1643821676592,
    "execution_millis": 2468,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 112
   },
   "source": "!wandb login 5bc70e88e5aac2b0cd097233114e75b47f6888e6 --relogin ",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ddb25dd7-890d-4fb8-88bb-69845c356094",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 104418,
    "execution_start": 1643821697595,
    "source_hash": "9263302b",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1367
   },
   "source": "# wandb.init(project=\"light_gbm\", entity=\"cz4041\",id='num_leaves_2_200_depth=-1')\nparams={ 'learning_rate':0.1,\n            'lambda_l1':0.1,\n            'lambda_l2':0.2,\n            'max_depth':-1,#default 6\n            'num_leaves':128 ,  # 大会更准,但可能过拟合\n            'objective':'multiclass',\n            'num_class':4, }\nwandb.config.update(params)\nmacro_F1_metric=[]\nfor model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n\n    # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n    # y_test_pred = model.predict(X_test).argmax(axis=1)\n    y_test_pred=model.predict(np.array(X)[test_index]).argmax(axis=1)\n    y_test=np.array(Y)[test_index]\n\n    score = accuracy_score(y_test, y_test_pred)\n    print(\"Accuracy score is \" + str(score))\n    report=classification_report(y_test, y_test_pred)\n    macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n    with open('LightGBM_data/LightGBM_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n        pickle.dump(model, f)#store as pickle\n    print('model saved to LightGBM_data/LightGBM_model_{}.pkl'.format(macro_F1_metric[-1]))\n    print(report)\n\n    # Build the plot\n#         cm = confusion_matrix(y_test, y_test_pred)\n#         plt.figure(figsize=(16,7))\n#         sb.set(font_scale=1.4)\n#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n\n#         plt.xlabel('Predicted label')\n#         plt.ylabel('True label')\n#         plt.title('Confusion Matrix for Random Forest Model')\n#         plt.show()\n# wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\nprint(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12586\n[LightGBM] [Info] Number of data points in the train set: 19187, number of used features: 72\n[LightGBM] [Info] Start training from score -1.384992\n[LightGBM] [Info] Start training from score -1.381044\n[LightGBM] [Info] Start training from score -1.387911\n[LightGBM] [Info] Start training from score -1.391258\n[1]\tvalid_0's multi_logloss: 1.23158\n[2]\tvalid_0's multi_logloss: 1.11204\n[3]\tvalid_0's multi_logloss: 1.01327\n[4]\tvalid_0's multi_logloss: 0.930998\n[5]\tvalid_0's multi_logloss: 0.860237\n[6]\tvalid_0's multi_logloss: 0.797342\n[7]\tvalid_0's multi_logloss: 0.740033\n[8]\tvalid_0's multi_logloss: 0.690954\n[9]\tvalid_0's multi_logloss: 0.646399\n[10]\tvalid_0's multi_logloss: 0.606179\n[11]\tvalid_0's multi_logloss: 0.571775\n[12]\tvalid_0's multi_logloss: 0.537309\n[13]\tvalid_0's multi_logloss: 0.509966\n[14]\tvalid_0's multi_logloss: 0.48328\n[15]\tvalid_0's multi_logloss: 0.457476\n[16]\tvalid_0's multi_logloss: 0.435352\n[17]\tvalid_0's multi_logloss: 0.414671\n[18]\tvalid_0's multi_logloss: 0.395115\n[19]\tvalid_0's multi_logloss: 0.378582\n[20]\tvalid_0's multi_logloss: 0.362011\n[21]\tvalid_0's multi_logloss: 0.348224\n[22]\tvalid_0's multi_logloss: 0.333537\n[23]\tvalid_0's multi_logloss: 0.32058\n[24]\tvalid_0's multi_logloss: 0.308052\n[25]\tvalid_0's multi_logloss: 0.296474\n[26]\tvalid_0's multi_logloss: 0.287202\n[27]\tvalid_0's multi_logloss: 0.276189\n[28]\tvalid_0's multi_logloss: 0.266634\n[29]\tvalid_0's multi_logloss: 0.259018\n[30]\tvalid_0's multi_logloss: 0.249479\n[31]\tvalid_0's multi_logloss: 0.241277\n[32]\tvalid_0's multi_logloss: 0.2321\n[33]\tvalid_0's multi_logloss: 0.225324\n[34]\tvalid_0's multi_logloss: 0.218325\n[35]\tvalid_0's multi_logloss: 0.211531\n[36]\tvalid_0's multi_logloss: 0.204994\n[37]\tvalid_0's multi_logloss: 0.199148\n[38]\tvalid_0's multi_logloss: 0.193576\n[39]\tvalid_0's multi_logloss: 0.187426\n[40]\tvalid_0's multi_logloss: 0.181597\n[41]\tvalid_0's multi_logloss: 0.177679\n[42]\tvalid_0's multi_logloss: 0.172985\n[43]\tvalid_0's multi_logloss: 0.167608\n[44]\tvalid_0's multi_logloss: 0.163722\n[45]\tvalid_0's multi_logloss: 0.159434\n[46]\tvalid_0's multi_logloss: 0.155528\n[47]\tvalid_0's multi_logloss: 0.151868\n[48]\tvalid_0's multi_logloss: 0.148917\n[49]\tvalid_0's multi_logloss: 0.144944\n[50]\tvalid_0's multi_logloss: 0.141289\n[51]\tvalid_0's multi_logloss: 0.13874\n[52]\tvalid_0's multi_logloss: 0.135333\n[53]\tvalid_0's multi_logloss: 0.133032\n[54]\tvalid_0's multi_logloss: 0.129908\n[55]\tvalid_0's multi_logloss: 0.127535\n[56]\tvalid_0's multi_logloss: 0.124541\n[57]\tvalid_0's multi_logloss: 0.12179\n[58]\tvalid_0's multi_logloss: 0.119667\n[59]\tvalid_0's multi_logloss: 0.117206\n[60]\tvalid_0's multi_logloss: 0.115231\n[61]\tvalid_0's multi_logloss: 0.112745\n[62]\tvalid_0's multi_logloss: 0.110505\n[63]\tvalid_0's multi_logloss: 0.10834\n[64]\tvalid_0's multi_logloss: 0.106384\n[65]\tvalid_0's multi_logloss: 0.104411\n[66]\tvalid_0's multi_logloss: 0.102901\n[67]\tvalid_0's multi_logloss: 0.101176\n[68]\tvalid_0's multi_logloss: 0.0992777\n[69]\tvalid_0's multi_logloss: 0.0975817\n[70]\tvalid_0's multi_logloss: 0.0959581\n[71]\tvalid_0's multi_logloss: 0.0943484\n[72]\tvalid_0's multi_logloss: 0.0928359\n[73]\tvalid_0's multi_logloss: 0.0913085\n[74]\tvalid_0's multi_logloss: 0.090187\n[75]\tvalid_0's multi_logloss: 0.0892864\n[76]\tvalid_0's multi_logloss: 0.0882725\n[77]\tvalid_0's multi_logloss: 0.0871258\n[78]\tvalid_0's multi_logloss: 0.0860339\n[79]\tvalid_0's multi_logloss: 0.084784\n[80]\tvalid_0's multi_logloss: 0.0838194\n[81]\tvalid_0's multi_logloss: 0.0826585\n[82]\tvalid_0's multi_logloss: 0.0815125\n[83]\tvalid_0's multi_logloss: 0.0805543\n[84]\tvalid_0's multi_logloss: 0.0796422\n[85]\tvalid_0's multi_logloss: 0.0787017\n[86]\tvalid_0's multi_logloss: 0.0779704\n[87]\tvalid_0's multi_logloss: 0.0771839\n[88]\tvalid_0's multi_logloss: 0.0764606\n[89]\tvalid_0's multi_logloss: 0.0758456\n[90]\tvalid_0's multi_logloss: 0.0750713\n[91]\tvalid_0's multi_logloss: 0.0745327\n[92]\tvalid_0's multi_logloss: 0.0739967\n[93]\tvalid_0's multi_logloss: 0.0734045\n[94]\tvalid_0's multi_logloss: 0.0727325\n[95]\tvalid_0's multi_logloss: 0.072183\n[96]\tvalid_0's multi_logloss: 0.0716499\n[97]\tvalid_0's multi_logloss: 0.071117\n[98]\tvalid_0's multi_logloss: 0.0704843\n[99]\tvalid_0's multi_logloss: 0.069896\n[100]\tvalid_0's multi_logloss: 0.0693965\nAccuracy score is 0.9826975192828852\nmodel saved to LightGBM_data/LightGBM_model_0.98.pkl\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1193\n           1       0.99      0.98      0.98      1174\n           2       0.98      0.98      0.98      1207\n           3       0.97      0.99      0.98      1223\n\n    accuracy                           0.98      4797\n   macro avg       0.98      0.98      0.98      4797\nweighted avg       0.98      0.98      0.98      4797\n\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010577 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12598\n[LightGBM] [Info] Number of data points in the train set: 19187, number of used features: 72\n[LightGBM] [Info] Start training from score -1.383328\n[LightGBM] [Info] Start training from score -1.396509\n[LightGBM] [Info] Start training from score -1.388120\n[LightGBM] [Info] Start training from score -1.377318\n[1]\tvalid_0's multi_logloss: 1.2348\n[2]\tvalid_0's multi_logloss: 1.11577\n[3]\tvalid_0's multi_logloss: 1.01859\n[4]\tvalid_0's multi_logloss: 0.935878\n[5]\tvalid_0's multi_logloss: 0.863005\n[6]\tvalid_0's multi_logloss: 0.799897\n[7]\tvalid_0's multi_logloss: 0.745\n[8]\tvalid_0's multi_logloss: 0.694328\n[9]\tvalid_0's multi_logloss: 0.649986\n[10]\tvalid_0's multi_logloss: 0.609061\n[11]\tvalid_0's multi_logloss: 0.573435\n[12]\tvalid_0's multi_logloss: 0.541657\n[13]\tvalid_0's multi_logloss: 0.512249\n[14]\tvalid_0's multi_logloss: 0.484804\n[15]\tvalid_0's multi_logloss: 0.461385\n[16]\tvalid_0's multi_logloss: 0.437561\n[17]\tvalid_0's multi_logloss: 0.418122\n[18]\tvalid_0's multi_logloss: 0.398465\n[19]\tvalid_0's multi_logloss: 0.379478\n[20]\tvalid_0's multi_logloss: 0.362761\n[21]\tvalid_0's multi_logloss: 0.348417\n[22]\tvalid_0's multi_logloss: 0.334224\n[23]\tvalid_0's multi_logloss: 0.322225\n[24]\tvalid_0's multi_logloss: 0.310579\n[25]\tvalid_0's multi_logloss: 0.299255\n[26]\tvalid_0's multi_logloss: 0.28766\n[27]\tvalid_0's multi_logloss: 0.277444\n[28]\tvalid_0's multi_logloss: 0.267462\n[29]\tvalid_0's multi_logloss: 0.25835\n[30]\tvalid_0's multi_logloss: 0.250112\n[31]\tvalid_0's multi_logloss: 0.243782\n[32]\tvalid_0's multi_logloss: 0.23655\n[33]\tvalid_0's multi_logloss: 0.228464\n[34]\tvalid_0's multi_logloss: 0.221599\n[35]\tvalid_0's multi_logloss: 0.214194\n[36]\tvalid_0's multi_logloss: 0.208286\n[37]\tvalid_0's multi_logloss: 0.202093\n[38]\tvalid_0's multi_logloss: 0.195884\n[39]\tvalid_0's multi_logloss: 0.19029\n[40]\tvalid_0's multi_logloss: 0.18519\n[41]\tvalid_0's multi_logloss: 0.179808\n[42]\tvalid_0's multi_logloss: 0.174239\n[43]\tvalid_0's multi_logloss: 0.169316\n[44]\tvalid_0's multi_logloss: 0.165682\n[45]\tvalid_0's multi_logloss: 0.161909\n[46]\tvalid_0's multi_logloss: 0.157525\n[47]\tvalid_0's multi_logloss: 0.153947\n[48]\tvalid_0's multi_logloss: 0.150341\n[49]\tvalid_0's multi_logloss: 0.147101\n[50]\tvalid_0's multi_logloss: 0.143524\n[51]\tvalid_0's multi_logloss: 0.140354\n[52]\tvalid_0's multi_logloss: 0.136975\n[53]\tvalid_0's multi_logloss: 0.133195\n[54]\tvalid_0's multi_logloss: 0.13044\n[55]\tvalid_0's multi_logloss: 0.126943\n[56]\tvalid_0's multi_logloss: 0.124433\n[57]\tvalid_0's multi_logloss: 0.121921\n[58]\tvalid_0's multi_logloss: 0.11933\n[59]\tvalid_0's multi_logloss: 0.117469\n[60]\tvalid_0's multi_logloss: 0.11561\n[61]\tvalid_0's multi_logloss: 0.113064\n[62]\tvalid_0's multi_logloss: 0.1109\n[63]\tvalid_0's multi_logloss: 0.109105\n[64]\tvalid_0's multi_logloss: 0.107231\n[65]\tvalid_0's multi_logloss: 0.105466\n[66]\tvalid_0's multi_logloss: 0.103588\n[67]\tvalid_0's multi_logloss: 0.102436\n[68]\tvalid_0's multi_logloss: 0.100623\n[69]\tvalid_0's multi_logloss: 0.0993902\n[70]\tvalid_0's multi_logloss: 0.0974527\n[71]\tvalid_0's multi_logloss: 0.0958876\n[72]\tvalid_0's multi_logloss: 0.0946398\n[73]\tvalid_0's multi_logloss: 0.0932337\n[74]\tvalid_0's multi_logloss: 0.0916629\n[75]\tvalid_0's multi_logloss: 0.0902585\n[76]\tvalid_0's multi_logloss: 0.0893183\n[77]\tvalid_0's multi_logloss: 0.0880026\n[78]\tvalid_0's multi_logloss: 0.0869001\n[79]\tvalid_0's multi_logloss: 0.0857702\n[80]\tvalid_0's multi_logloss: 0.0847049\n[81]\tvalid_0's multi_logloss: 0.08368\n[82]\tvalid_0's multi_logloss: 0.0826248\n[83]\tvalid_0's multi_logloss: 0.0815979\n[84]\tvalid_0's multi_logloss: 0.080811\n[85]\tvalid_0's multi_logloss: 0.0798334\n[86]\tvalid_0's multi_logloss: 0.0788825\n[87]\tvalid_0's multi_logloss: 0.0782222\n[88]\tvalid_0's multi_logloss: 0.0772974\n[89]\tvalid_0's multi_logloss: 0.0762818\n[90]\tvalid_0's multi_logloss: 0.075301\n[91]\tvalid_0's multi_logloss: 0.0745841\n[92]\tvalid_0's multi_logloss: 0.0739162\n[93]\tvalid_0's multi_logloss: 0.0729323\n[94]\tvalid_0's multi_logloss: 0.0724795\n[95]\tvalid_0's multi_logloss: 0.0719581\n[96]\tvalid_0's multi_logloss: 0.0713575\n[97]\tvalid_0's multi_logloss: 0.0706974\n[98]\tvalid_0's multi_logloss: 0.070107\n[99]\tvalid_0's multi_logloss: 0.0696157\n[100]\tvalid_0's multi_logloss: 0.0693622\nAccuracy score is 0.9820721284135918\nmodel saved to LightGBM_data/LightGBM_model_0.98.pkl\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.99      1185\n           1       0.98      0.97      0.98      1248\n           2       0.99      0.98      0.98      1208\n           3       0.98      0.99      0.98      1156\n\n    accuracy                           0.98      4797\n   macro avg       0.98      0.98      0.98      4797\nweighted avg       0.98      0.98      0.98      4797\n\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009981 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12563\n[LightGBM] [Info] Number of data points in the train set: 19187, number of used features: 72\n[LightGBM] [Info] Start training from score -1.396509\n[LightGBM] [Info] Start training from score -1.375667\n[LightGBM] [Info] Start training from score -1.385409\n[LightGBM] [Info] Start training from score -1.387703\n[1]\tvalid_0's multi_logloss: 1.23294\n[2]\tvalid_0's multi_logloss: 1.11351\n[3]\tvalid_0's multi_logloss: 1.01543\n[4]\tvalid_0's multi_logloss: 0.932405\n[5]\tvalid_0's multi_logloss: 0.86247\n[6]\tvalid_0's multi_logloss: 0.802076\n[7]\tvalid_0's multi_logloss: 0.746895\n[8]\tvalid_0's multi_logloss: 0.698421\n[9]\tvalid_0's multi_logloss: 0.654735\n[10]\tvalid_0's multi_logloss: 0.612336\n[11]\tvalid_0's multi_logloss: 0.578788\n[12]\tvalid_0's multi_logloss: 0.545589\n[13]\tvalid_0's multi_logloss: 0.516626\n[14]\tvalid_0's multi_logloss: 0.490226\n[15]\tvalid_0's multi_logloss: 0.46454\n[16]\tvalid_0's multi_logloss: 0.440102\n[17]\tvalid_0's multi_logloss: 0.419562\n[18]\tvalid_0's multi_logloss: 0.40143\n[19]\tvalid_0's multi_logloss: 0.384935\n[20]\tvalid_0's multi_logloss: 0.367797\n[21]\tvalid_0's multi_logloss: 0.353216\n[22]\tvalid_0's multi_logloss: 0.337329\n[23]\tvalid_0's multi_logloss: 0.323868\n[24]\tvalid_0's multi_logloss: 0.311575\n[25]\tvalid_0's multi_logloss: 0.299691\n[26]\tvalid_0's multi_logloss: 0.28835\n[27]\tvalid_0's multi_logloss: 0.278365\n[28]\tvalid_0's multi_logloss: 0.269065\n[29]\tvalid_0's multi_logloss: 0.260452\n[30]\tvalid_0's multi_logloss: 0.250542\n[31]\tvalid_0's multi_logloss: 0.24263\n[32]\tvalid_0's multi_logloss: 0.233587\n[33]\tvalid_0's multi_logloss: 0.226694\n[34]\tvalid_0's multi_logloss: 0.219292\n[35]\tvalid_0's multi_logloss: 0.213118\n[36]\tvalid_0's multi_logloss: 0.206448\n[37]\tvalid_0's multi_logloss: 0.200574\n[38]\tvalid_0's multi_logloss: 0.195413\n[39]\tvalid_0's multi_logloss: 0.19027\n[40]\tvalid_0's multi_logloss: 0.185188\n[41]\tvalid_0's multi_logloss: 0.180432\n[42]\tvalid_0's multi_logloss: 0.175427\n[43]\tvalid_0's multi_logloss: 0.170751\n[44]\tvalid_0's multi_logloss: 0.166725\n[45]\tvalid_0's multi_logloss: 0.162589\n[46]\tvalid_0's multi_logloss: 0.157732\n[47]\tvalid_0's multi_logloss: 0.154055\n[48]\tvalid_0's multi_logloss: 0.151002\n[49]\tvalid_0's multi_logloss: 0.147178\n[50]\tvalid_0's multi_logloss: 0.143659\n[51]\tvalid_0's multi_logloss: 0.139839\n[52]\tvalid_0's multi_logloss: 0.137349\n[53]\tvalid_0's multi_logloss: 0.133982\n[54]\tvalid_0's multi_logloss: 0.131425\n[55]\tvalid_0's multi_logloss: 0.128388\n[56]\tvalid_0's multi_logloss: 0.126019\n[57]\tvalid_0's multi_logloss: 0.123161\n[58]\tvalid_0's multi_logloss: 0.121022\n[59]\tvalid_0's multi_logloss: 0.118174\n[60]\tvalid_0's multi_logloss: 0.115442\n[61]\tvalid_0's multi_logloss: 0.113523\n[62]\tvalid_0's multi_logloss: 0.111418\n[63]\tvalid_0's multi_logloss: 0.109586\n[64]\tvalid_0's multi_logloss: 0.108083\n[65]\tvalid_0's multi_logloss: 0.106145\n[66]\tvalid_0's multi_logloss: 0.10442\n[67]\tvalid_0's multi_logloss: 0.103189\n[68]\tvalid_0's multi_logloss: 0.101505\n[69]\tvalid_0's multi_logloss: 0.0998584\n[70]\tvalid_0's multi_logloss: 0.0988058\n[71]\tvalid_0's multi_logloss: 0.096802\n[72]\tvalid_0's multi_logloss: 0.0956881\n[73]\tvalid_0's multi_logloss: 0.0941573\n[74]\tvalid_0's multi_logloss: 0.0928311\n[75]\tvalid_0's multi_logloss: 0.0914271\n[76]\tvalid_0's multi_logloss: 0.0902967\n[77]\tvalid_0's multi_logloss: 0.0890518\n[78]\tvalid_0's multi_logloss: 0.0878509\n[79]\tvalid_0's multi_logloss: 0.0866682\n[80]\tvalid_0's multi_logloss: 0.0853852\n[81]\tvalid_0's multi_logloss: 0.0843064\n[82]\tvalid_0's multi_logloss: 0.0832548\n[83]\tvalid_0's multi_logloss: 0.0825089\n[84]\tvalid_0's multi_logloss: 0.0816181\n[85]\tvalid_0's multi_logloss: 0.080587\n[86]\tvalid_0's multi_logloss: 0.0798635\n[87]\tvalid_0's multi_logloss: 0.0790289\n[88]\tvalid_0's multi_logloss: 0.0785249\n[89]\tvalid_0's multi_logloss: 0.0774459\n[90]\tvalid_0's multi_logloss: 0.0764217\n[91]\tvalid_0's multi_logloss: 0.0754911\n[92]\tvalid_0's multi_logloss: 0.0749935\n[93]\tvalid_0's multi_logloss: 0.074143\n[94]\tvalid_0's multi_logloss: 0.0734287\n[95]\tvalid_0's multi_logloss: 0.072939\n[96]\tvalid_0's multi_logloss: 0.0723704\n[97]\tvalid_0's multi_logloss: 0.0714168\n[98]\tvalid_0's multi_logloss: 0.070719\n[99]\tvalid_0's multi_logloss: 0.0701708\n[100]\tvalid_0's multi_logloss: 0.0696127\nAccuracy score is 0.9835313737752762\nmodel saved to LightGBM_data/LightGBM_model_0.98.pkl\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1248\n           1       0.98      0.97      0.98      1148\n           2       0.98      0.98      0.98      1195\n           3       0.98      0.99      0.98      1206\n\n    accuracy                           0.98      4797\n   macro avg       0.98      0.98      0.98      4797\nweighted avg       0.98      0.98      0.98      4797\n\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009638 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12586\n[LightGBM] [Info] Number of data points in the train set: 19187, number of used features: 72\n[LightGBM] [Info] Start training from score -1.383120\n[LightGBM] [Info] Start training from score -1.390630\n[LightGBM] [Info] Start training from score -1.388747\n[LightGBM] [Info] Start training from score -1.382705\n[1]\tvalid_0's multi_logloss: 1.23466\n[2]\tvalid_0's multi_logloss: 1.11626\n[3]\tvalid_0's multi_logloss: 1.02105\n[4]\tvalid_0's multi_logloss: 0.939372\n[5]\tvalid_0's multi_logloss: 0.868327\n[6]\tvalid_0's multi_logloss: 0.807747\n[7]\tvalid_0's multi_logloss: 0.752477\n[8]\tvalid_0's multi_logloss: 0.704427\n[9]\tvalid_0's multi_logloss: 0.658777\n[10]\tvalid_0's multi_logloss: 0.617563\n[11]\tvalid_0's multi_logloss: 0.583332\n[12]\tvalid_0's multi_logloss: 0.549472\n[13]\tvalid_0's multi_logloss: 0.520301\n[14]\tvalid_0's multi_logloss: 0.491399\n[15]\tvalid_0's multi_logloss: 0.467794\n[16]\tvalid_0's multi_logloss: 0.445642\n[17]\tvalid_0's multi_logloss: 0.421668\n[18]\tvalid_0's multi_logloss: 0.400387\n[19]\tvalid_0's multi_logloss: 0.381913\n[20]\tvalid_0's multi_logloss: 0.364273\n[21]\tvalid_0's multi_logloss: 0.347975\n[22]\tvalid_0's multi_logloss: 0.333662\n[23]\tvalid_0's multi_logloss: 0.323253\n[24]\tvalid_0's multi_logloss: 0.309889\n[25]\tvalid_0's multi_logloss: 0.298948\n[26]\tvalid_0's multi_logloss: 0.287574\n[27]\tvalid_0's multi_logloss: 0.277961\n[28]\tvalid_0's multi_logloss: 0.269678\n[29]\tvalid_0's multi_logloss: 0.260142\n[30]\tvalid_0's multi_logloss: 0.251741\n[31]\tvalid_0's multi_logloss: 0.243826\n[32]\tvalid_0's multi_logloss: 0.236721\n[33]\tvalid_0's multi_logloss: 0.229445\n[34]\tvalid_0's multi_logloss: 0.222384\n[35]\tvalid_0's multi_logloss: 0.216779\n[36]\tvalid_0's multi_logloss: 0.211126\n[37]\tvalid_0's multi_logloss: 0.20363\n[38]\tvalid_0's multi_logloss: 0.197971\n[39]\tvalid_0's multi_logloss: 0.192871\n[40]\tvalid_0's multi_logloss: 0.187234\n[41]\tvalid_0's multi_logloss: 0.181899\n[42]\tvalid_0's multi_logloss: 0.177813\n[43]\tvalid_0's multi_logloss: 0.173829\n[44]\tvalid_0's multi_logloss: 0.170548\n[45]\tvalid_0's multi_logloss: 0.166375\n[46]\tvalid_0's multi_logloss: 0.162316\n[47]\tvalid_0's multi_logloss: 0.158596\n[48]\tvalid_0's multi_logloss: 0.154369\n[49]\tvalid_0's multi_logloss: 0.151051\n[50]\tvalid_0's multi_logloss: 0.14759\n[51]\tvalid_0's multi_logloss: 0.144739\n[52]\tvalid_0's multi_logloss: 0.141357\n[53]\tvalid_0's multi_logloss: 0.138353\n[54]\tvalid_0's multi_logloss: 0.135617\n[55]\tvalid_0's multi_logloss: 0.13283\n[56]\tvalid_0's multi_logloss: 0.129739\n[57]\tvalid_0's multi_logloss: 0.127002\n[58]\tvalid_0's multi_logloss: 0.124771\n[59]\tvalid_0's multi_logloss: 0.122659\n[60]\tvalid_0's multi_logloss: 0.120668\n[61]\tvalid_0's multi_logloss: 0.118128\n[62]\tvalid_0's multi_logloss: 0.116382\n[63]\tvalid_0's multi_logloss: 0.114497\n[64]\tvalid_0's multi_logloss: 0.112192\n[65]\tvalid_0's multi_logloss: 0.110307\n[66]\tvalid_0's multi_logloss: 0.108488\n[67]\tvalid_0's multi_logloss: 0.106998\n[68]\tvalid_0's multi_logloss: 0.105281\n[69]\tvalid_0's multi_logloss: 0.10346\n[70]\tvalid_0's multi_logloss: 0.101901\n[71]\tvalid_0's multi_logloss: 0.100599\n[72]\tvalid_0's multi_logloss: 0.0988156\n[73]\tvalid_0's multi_logloss: 0.0972267\n[74]\tvalid_0's multi_logloss: 0.0959929\n[75]\tvalid_0's multi_logloss: 0.0946834\n[76]\tvalid_0's multi_logloss: 0.093313\n[77]\tvalid_0's multi_logloss: 0.0921098\n[78]\tvalid_0's multi_logloss: 0.0910387\n[79]\tvalid_0's multi_logloss: 0.0897554\n[80]\tvalid_0's multi_logloss: 0.0885358\n[81]\tvalid_0's multi_logloss: 0.0878369\n[82]\tvalid_0's multi_logloss: 0.0868549\n[83]\tvalid_0's multi_logloss: 0.0859242\n[84]\tvalid_0's multi_logloss: 0.0852257\n[85]\tvalid_0's multi_logloss: 0.0840616\n[86]\tvalid_0's multi_logloss: 0.0830892\n[87]\tvalid_0's multi_logloss: 0.0819132\n[88]\tvalid_0's multi_logloss: 0.0812581\n[89]\tvalid_0's multi_logloss: 0.080586\n[90]\tvalid_0's multi_logloss: 0.0798244\n[91]\tvalid_0's multi_logloss: 0.0791257\n[92]\tvalid_0's multi_logloss: 0.0786635\n[93]\tvalid_0's multi_logloss: 0.0781226\n[94]\tvalid_0's multi_logloss: 0.0776526\n[95]\tvalid_0's multi_logloss: 0.0770389\n[96]\tvalid_0's multi_logloss: 0.0765071\n[97]\tvalid_0's multi_logloss: 0.0759501\n[98]\tvalid_0's multi_logloss: 0.0752208\n[99]\tvalid_0's multi_logloss: 0.0747292\n[100]\tvalid_0's multi_logloss: 0.074181\nAccuracy score is 0.980195955805712\nmodel saved to LightGBM_data/LightGBM_model_0.98.pkl\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1184\n           1       0.98      0.97      0.97      1220\n           2       0.98      0.98      0.98      1211\n           3       0.97      0.99      0.98      1182\n\n    accuracy                           0.98      4797\n   macro avg       0.98      0.98      0.98      4797\nweighted avg       0.98      0.98      0.98      4797\n\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017234 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 12573\n[LightGBM] [Info] Number of data points in the train set: 19188, number of used features: 72\n[LightGBM] [Info] Start training from score -1.383588\n[LightGBM] [Info] Start training from score -1.387755\n[LightGBM] [Info] Start training from score -1.381304\n[LightGBM] [Info] Start training from score -1.392568\n[1]\tvalid_0's multi_logloss: 1.2341\n[2]\tvalid_0's multi_logloss: 1.11104\n[3]\tvalid_0's multi_logloss: 1.01036\n[4]\tvalid_0's multi_logloss: 0.925647\n[5]\tvalid_0's multi_logloss: 0.852736\n[6]\tvalid_0's multi_logloss: 0.79076\n[7]\tvalid_0's multi_logloss: 0.734006\n[8]\tvalid_0's multi_logloss: 0.681483\n[9]\tvalid_0's multi_logloss: 0.634611\n[10]\tvalid_0's multi_logloss: 0.593229\n[11]\tvalid_0's multi_logloss: 0.558009\n[12]\tvalid_0's multi_logloss: 0.528408\n[13]\tvalid_0's multi_logloss: 0.501037\n[14]\tvalid_0's multi_logloss: 0.473679\n[15]\tvalid_0's multi_logloss: 0.449051\n[16]\tvalid_0's multi_logloss: 0.427061\n[17]\tvalid_0's multi_logloss: 0.40751\n[18]\tvalid_0's multi_logloss: 0.387224\n[19]\tvalid_0's multi_logloss: 0.369275\n[20]\tvalid_0's multi_logloss: 0.353809\n[21]\tvalid_0's multi_logloss: 0.338903\n[22]\tvalid_0's multi_logloss: 0.324291\n[23]\tvalid_0's multi_logloss: 0.310274\n[24]\tvalid_0's multi_logloss: 0.298687\n[25]\tvalid_0's multi_logloss: 0.287856\n[26]\tvalid_0's multi_logloss: 0.27773\n[27]\tvalid_0's multi_logloss: 0.268162\n[28]\tvalid_0's multi_logloss: 0.260474\n[29]\tvalid_0's multi_logloss: 0.25176\n[30]\tvalid_0's multi_logloss: 0.242555\n[31]\tvalid_0's multi_logloss: 0.235317\n[32]\tvalid_0's multi_logloss: 0.226848\n[33]\tvalid_0's multi_logloss: 0.219278\n[34]\tvalid_0's multi_logloss: 0.212617\n[35]\tvalid_0's multi_logloss: 0.205443\n[36]\tvalid_0's multi_logloss: 0.200051\n[37]\tvalid_0's multi_logloss: 0.195406\n[38]\tvalid_0's multi_logloss: 0.189533\n[39]\tvalid_0's multi_logloss: 0.18426\n[40]\tvalid_0's multi_logloss: 0.17936\n[41]\tvalid_0's multi_logloss: 0.173546\n[42]\tvalid_0's multi_logloss: 0.168342\n[43]\tvalid_0's multi_logloss: 0.16415\n[44]\tvalid_0's multi_logloss: 0.160465\n[45]\tvalid_0's multi_logloss: 0.15677\n[46]\tvalid_0's multi_logloss: 0.152754\n[47]\tvalid_0's multi_logloss: 0.149087\n[48]\tvalid_0's multi_logloss: 0.145045\n[49]\tvalid_0's multi_logloss: 0.141541\n[50]\tvalid_0's multi_logloss: 0.138677\n[51]\tvalid_0's multi_logloss: 0.135037\n[52]\tvalid_0's multi_logloss: 0.131392\n[53]\tvalid_0's multi_logloss: 0.128465\n[54]\tvalid_0's multi_logloss: 0.125689\n[55]\tvalid_0's multi_logloss: 0.122573\n[56]\tvalid_0's multi_logloss: 0.120477\n[57]\tvalid_0's multi_logloss: 0.118207\n[58]\tvalid_0's multi_logloss: 0.115822\n[59]\tvalid_0's multi_logloss: 0.113414\n[60]\tvalid_0's multi_logloss: 0.111035\n[61]\tvalid_0's multi_logloss: 0.108905\n[62]\tvalid_0's multi_logloss: 0.106815\n[63]\tvalid_0's multi_logloss: 0.10466\n[64]\tvalid_0's multi_logloss: 0.102726\n[65]\tvalid_0's multi_logloss: 0.10092\n[66]\tvalid_0's multi_logloss: 0.09899\n[67]\tvalid_0's multi_logloss: 0.0973203\n[68]\tvalid_0's multi_logloss: 0.0954744\n[69]\tvalid_0's multi_logloss: 0.0940708\n[70]\tvalid_0's multi_logloss: 0.0928582\n[71]\tvalid_0's multi_logloss: 0.0913613\n[72]\tvalid_0's multi_logloss: 0.0901151\n[73]\tvalid_0's multi_logloss: 0.088474\n[74]\tvalid_0's multi_logloss: 0.0873986\n[75]\tvalid_0's multi_logloss: 0.0861892\n[76]\tvalid_0's multi_logloss: 0.0850155\n[77]\tvalid_0's multi_logloss: 0.0838869\n[78]\tvalid_0's multi_logloss: 0.0825488\n[79]\tvalid_0's multi_logloss: 0.0815054\n[80]\tvalid_0's multi_logloss: 0.0804264\n[81]\tvalid_0's multi_logloss: 0.0792884\n[82]\tvalid_0's multi_logloss: 0.078448\n[83]\tvalid_0's multi_logloss: 0.0772428\n[84]\tvalid_0's multi_logloss: 0.0762621\n[85]\tvalid_0's multi_logloss: 0.0752773\n[86]\tvalid_0's multi_logloss: 0.0745589\n[87]\tvalid_0's multi_logloss: 0.0736915\n[88]\tvalid_0's multi_logloss: 0.0729021\n[89]\tvalid_0's multi_logloss: 0.0721295\n[90]\tvalid_0's multi_logloss: 0.0713097\n[91]\tvalid_0's multi_logloss: 0.0706519\n[92]\tvalid_0's multi_logloss: 0.0699521\n[93]\tvalid_0's multi_logloss: 0.0692492\n[94]\tvalid_0's multi_logloss: 0.0685638\n[95]\tvalid_0's multi_logloss: 0.0681344\n[96]\tvalid_0's multi_logloss: 0.0676671\n[97]\tvalid_0's multi_logloss: 0.0671213\n[98]\tvalid_0's multi_logloss: 0.0663826\n[99]\tvalid_0's multi_logloss: 0.0660379\n[100]\tvalid_0's multi_logloss: 0.0655077\nAccuracy score is 0.982068390325271\nmodel saved to LightGBM_data/LightGBM_model_0.98.pkl\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1186\n           1       0.98      0.96      0.97      1206\n           2       0.98      0.98      0.98      1175\n           3       0.97      0.99      0.98      1229\n\n    accuracy                           0.98      4796\n   macro avg       0.98      0.98      0.98      4796\nweighted avg       0.98      0.98      0.98      4796\n\navg macro_F1_metric: 0.9800000000000001\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "30061443-9be9-4089-bd23-f45f0e027dae",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1985,
    "execution_start": 1643822109565,
    "source_hash": "e0ba73e9",
    "tags": [],
    "owner_user_id": "b8794142-6b09-4e0d-b24a-7348839ee3c8",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 300.1875,
    "deepnote_output_heights": [
     null,
     21.1875
    ]
   },
   "source": "with open(('LightGBM_data/LightGBM_model_0.98.pkl'), 'rb+') as f:\n    model = pickle.load(f)#load model\ngenerate_sumbit_csv(model,'lightbgm_upsampling')",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "save to data/test_submit_lightbgm_upsampling_02-02-2022_17_15_11_508765.csv\ndistribution:\n1 1196\n2 3527\n3 1385\n4 17748\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 12,
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "aa55e874-7ed3-4379-8b54-b9df2aa9b970",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 3,
    "execution_start": 1643734960310,
    "source_hash": "f38e2229",
    "tags": [],
    "owner_user_id": "b5b89f4c-bc16-482e-95c0-17e3550ab959",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 693
   },
   "source": "# #########################################################################################\n# # 模型 lightgbm\n# #########################################################################################\n\n# # 准备数据\n# X = data_df.iloc[:, :-1]\n# Y = data_df[\"label\"]\n# X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\n# # X_valid, X_test, y_valid, y_test = train_test_split(X_test,y_test,test_size=0.5)\n# # 训练\n# btime = datetime.datetime.now() \n# clf=light_bgm_train(X_train,y_train,X_test,y_test)\n# print ('all tasks done. total time used:%s s.\\n\\n'%((datetime.datetime.now() - btime).total_seconds()))\n\n# # 1、AUC\n# y_pred_pa = clf.predict(X_test)  # !!!注意lgm预测的是分数，类似 sklearn的predict_proba\n# # y_test_oh = label_binarize(y_test, classes= [0,1,2,3])\n# # print ('调用函数auc：', roc_auc_score(y_test_oh, y_pred_pa, average='micro'))\n\n# #  2、混淆矩阵\n# y_pred = y_pred_pa.argmax(axis=1)\n# confusion_matrix(y_test, y_pred)\n\n# #  3、经典-精确率、召回率、F1分数\n# precision_score(y_test, y_pred,average='micro')\n# recall_score(y_test, y_pred,average='micro')\n# f1_score(y_test, y_pred,average='micro')\n\n# # 4、模型报告\n# print(classification_report(y_test, y_pred))\n\n# # 保存模型\n# with open('LightGBM_data/LightGBM_model.pkl', 'wb') as f:\n#     pickle.dump(clf, f)\n#     print('model saved')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f92fbbcf-fa92-4caa-ad57-5cd0b8c2766a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "f1b44821-8559-440b-9469-fc434886f33c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 }
}