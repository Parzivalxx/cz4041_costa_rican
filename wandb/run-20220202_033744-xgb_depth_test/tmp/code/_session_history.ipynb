{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183880d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import torch\n",
    "# from torch import nn\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV , KFold, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import wandb\n",
    "import pickle\n",
    "from utils import *\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "df = pd.read_csv('data/train_processed.csv')\n",
    "target = 'Target'\n",
    "IDcol = 'Id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebca1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e95f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import torch\n",
    "# from torch import nn\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV , KFold, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import wandb\n",
    "import pickle\n",
    "from utils import *\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e4cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install imblearn\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c174191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import torch\n",
    "# from torch import nn\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV , KFold, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import wandb\n",
    "import pickle\n",
    "from utils import *\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e00b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature0  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
      "0     0.302245       0.0       0.2       0.0       1.0       1.0       0.0   \n",
      "1    -0.224298       0.0       0.3       0.0       1.0       1.0       1.0   \n",
      "2     0.206510       0.0       0.7       0.0       1.0       1.0       0.0   \n",
      "3     0.206510       0.0       0.4       0.0       1.0       1.0       1.0   \n",
      "4     0.206510       0.0       0.4       0.0       1.0       1.0       1.0   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9552 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n",
      "9553 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n",
      "9554 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n",
      "9555 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n",
      "9556 -0.750840       0.0       0.5       0.0       1.0       1.0       0.0   \n",
      "\n",
      "      feature7  feature8  feature9  ...  feature63  feature64  feature65  \\\n",
      "0          0.0       0.0     0.125  ...          0          0          2   \n",
      "1          0.0       0.0     0.125  ...          0          0          1   \n",
      "2          0.0       0.0     0.000  ...          0          0          1   \n",
      "3          0.0       0.0     0.250  ...          0          0          1   \n",
      "4          0.0       0.0     0.250  ...          0          0          1   \n",
      "...        ...       ...       ...  ...        ...        ...        ...   \n",
      "9552       0.0       0.0     0.250  ...          5          1          2   \n",
      "9553       0.0       0.0     0.250  ...          5          1          2   \n",
      "9554       0.0       0.0     0.250  ...          5          1          2   \n",
      "9555       0.0       0.0     0.250  ...          5          1          2   \n",
      "9556       0.0       0.0     0.250  ...          5          1          2   \n",
      "\n",
      "      feature66  feature67  feature68  feature69  feature70  feature71  label  \n",
      "0             1          0          0          1          0          0      3  \n",
      "1             1          4          5          0          0          0      3  \n",
      "2             1          4          0          0          0          0      3  \n",
      "3             1          0          0          0          0          0      3  \n",
      "4             1          0          0          0          0          0      3  \n",
      "...         ...        ...        ...        ...        ...        ...    ...  \n",
      "9552          2          0          1          0          0          3      1  \n",
      "9553          2          0          1          0          0          3      1  \n",
      "9554          2          0          1          0          0          3      1  \n",
      "9555          2          0          1          0          0          3      1  \n",
      "9556          2          0          1          0          0          3      1  \n",
      "\n",
      "[9557 rows x 73 columns]"
     ]
    }
   ],
   "source": [
    "data_df=load_processed_csv(train=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d1f2acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature0  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
      "0  0.302245       0.0       0.2       0.0       1.0       1.0       0.0   \n",
      "1 -0.224298       0.0       0.3       0.0       1.0       1.0       1.0   \n",
      "2  0.206510       0.0       0.7       0.0       1.0       1.0       0.0   \n",
      "3  0.206510       0.0       0.4       0.0       1.0       1.0       1.0   \n",
      "4  0.206510       0.0       0.4       0.0       1.0       1.0       1.0   \n",
      "\n",
      "   feature7  feature8  feature9  ...  feature63  feature64  feature65  \\\n",
      "0       0.0       0.0     0.125  ...          0          0          2   \n",
      "1       0.0       0.0     0.125  ...          0          0          1   \n",
      "2       0.0       0.0     0.000  ...          0          0          1   \n",
      "3       0.0       0.0     0.250  ...          0          0          1   \n",
      "4       0.0       0.0     0.250  ...          0          0          1   \n",
      "\n",
      "   feature66  feature67  feature68  feature69  feature70  feature71  label  \n",
      "0          1          0          0          1          0          0      3  \n",
      "1          1          4          5          0          0          0      3  \n",
      "2          1          4          0          0          0          0      3  \n",
      "3          1          0          0          0          0          0      3  \n",
      "4          1          0          0          0          0          0      3  \n",
      "\n",
      "[5 rows x 73 columns]"
     ]
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a49c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AxesSubplot:>"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "sb.heatmap(data_df.corr(), xticklabels=data_df.corr().columns, yticklabels=data_df.corr().columns, cmap='RdYlGn', center=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aede28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params_xgb, dtrain , 700, vals=[(dtrain,'eval'),(dtest, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a84516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df.iloc[:, :-1]\n",
    "Y = data_df[\"label\"]\n",
    "print(\"original distribution:\",Counter(Y))\n",
    "#upsamlping \n",
    "smo = SMOTE(random_state=42)\n",
    "X_smo, Y_smo = smo.fit_resample(X, Y)\n",
    "print(\"smote distribution:\",Counter(Y_smo))\n",
    "X,Y=X_smo, Y_smo#using smote dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994626b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"5bc70e88e5aac2b0cd097233114e75b47f6888e6\")\n",
    "wandb.init(project=\"test-project\", entity=\"cz4041\",id='xgb_depth_test')\n",
    "for depth in range(2,30):\n",
    "    params={ 'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':depth, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2}\n",
    "    wandb.config.update(params)\n",
    "    macro_F1_metric=[]\n",
    "    for model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n",
    "        # y_test_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_test_pred=model.predict(np.array(X)[test_index]).argmax(axis=1)\n",
    "        y_test=np.array(Y)[test_index]\n",
    "\n",
    "        score = accuracy_score(y_test, y_test_pred)\n",
    "        print(\"Accuracy score is \" + str(score))\n",
    "        report=classification_report(y_test, y_test_pred)\n",
    "        macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n",
    "        with open('xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n",
    "            pickle.dump(model, f)#store as pickle\n",
    "        print('model saved to xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]))\n",
    "        print(report)\n",
    "\n",
    "        # Build the plot\n",
    "#         cm = confusion_matrix(y_test, y_test_pred)\n",
    "#         plt.figure(figsize=(16,7))\n",
    "#         sb.set(font_scale=1.4)\n",
    "#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n",
    "\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title('Confusion Matrix for Random Forest Model')\n",
    "#         plt.show()\n",
    "    wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\n",
    "    print(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric),depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4364ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params, dtrain , 700, vals=[(dtrain,'eval'),(dtest, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb125bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"5bc70e88e5aac2b0cd097233114e75b47f6888e6\")\n",
    "wandb.init(project=\"test-project\", entity=\"cz4041\",id='xgb_depth_test')\n",
    "for depth in range(2,30):\n",
    "    params={ 'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':depth, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2}\n",
    "    wandb.config.update(params)\n",
    "    macro_F1_metric=[]\n",
    "    for model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n",
    "        # y_test_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_test_pred=model.predict(np.array(X)[test_index]).argmax(axis=1)\n",
    "        y_test=np.array(Y)[test_index]\n",
    "\n",
    "        score = accuracy_score(y_test, y_test_pred)\n",
    "        print(\"Accuracy score is \" + str(score))\n",
    "        report=classification_report(y_test, y_test_pred)\n",
    "        macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n",
    "        with open('xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n",
    "            pickle.dump(model, f)#store as pickle\n",
    "        print('model saved to xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]))\n",
    "        print(report)\n",
    "\n",
    "        # Build the plot\n",
    "#         cm = confusion_matrix(y_test, y_test_pred)\n",
    "#         plt.figure(figsize=(16,7))\n",
    "#         sb.set(font_scale=1.4)\n",
    "#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n",
    "\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title('Confusion Matrix for Random Forest Model')\n",
    "#         plt.show()\n",
    "    wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\n",
    "    print(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric),depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a3e84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params, dtrain , 700, vals=[(train_data,'eval'),(validation_data, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "588fcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"5bc70e88e5aac2b0cd097233114e75b47f6888e6\")\n",
    "wandb.init(project=\"test-project\", entity=\"cz4041\",id='xgb_depth_test')\n",
    "for depth in range(2,30):\n",
    "    params={ 'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':depth, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2}\n",
    "    wandb.config.update(params)\n",
    "    macro_F1_metric=[]\n",
    "    for model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n",
    "        # y_test_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_test_pred=model.predict(np.array(X)[test_index]).argmax(axis=1)\n",
    "        y_test=np.array(Y)[test_index]\n",
    "\n",
    "        score = accuracy_score(y_test, y_test_pred)\n",
    "        print(\"Accuracy score is \" + str(score))\n",
    "        report=classification_report(y_test, y_test_pred)\n",
    "        macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n",
    "        with open('xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n",
    "            pickle.dump(model, f)#store as pickle\n",
    "        print('model saved to xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]))\n",
    "        print(report)\n",
    "\n",
    "        # Build the plot\n",
    "#         cm = confusion_matrix(y_test, y_test_pred)\n",
    "#         plt.figure(figsize=(16,7))\n",
    "#         sb.set(font_scale=1.4)\n",
    "#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n",
    "\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title('Confusion Matrix for Random Forest Model')\n",
    "#         plt.show()\n",
    "    wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\n",
    "    print(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric),depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e1ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params, train_data , 700, vals=[(train_data,'eval'),(validation_data, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f50ef416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params, train_data , 700, vals=[(train_data,'eval'),(validation_data, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f76defdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"5bc70e88e5aac2b0cd097233114e75b47f6888e6\")\n",
    "wandb.init(project=\"test-project\", entity=\"cz4041\",id='xgb_depth_test')\n",
    "for depth in range(2,30):\n",
    "    params={ 'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':depth, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2}\n",
    "    wandb.config.update(params)\n",
    "    macro_F1_metric=[]\n",
    "    for model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n",
    "        # y_test_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_test_pred=model.predict(np.array(X)[test_index]).argmax(axis=1)\n",
    "        y_test=np.array(Y)[test_index]\n",
    "\n",
    "        score = accuracy_score(y_test, y_test_pred)\n",
    "        print(\"Accuracy score is \" + str(score))\n",
    "        report=classification_report(y_test, y_test_pred)\n",
    "        macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n",
    "        with open('xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n",
    "            pickle.dump(model, f)#store as pickle\n",
    "        print('model saved to xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]))\n",
    "        print(report)\n",
    "\n",
    "        # Build the plot\n",
    "#         cm = confusion_matrix(y_test, y_test_pred)\n",
    "#         plt.figure(figsize=(16,7))\n",
    "#         sb.set(font_scale=1.4)\n",
    "#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n",
    "\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title('Confusion Matrix for Random Forest Model')\n",
    "#         plt.show()\n",
    "    wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\n",
    "    print(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric),depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5153ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params, train_data , 700, evals=[(dtrain,'eval'),(dtest, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d572976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"5bc70e88e5aac2b0cd097233114e75b47f6888e6\")\n",
    "wandb.init(project=\"test-project\", entity=\"cz4041\",id='xgb_depth_test')\n",
    "for depth in range(2,30):\n",
    "    params={ 'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':depth, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2}\n",
    "    wandb.config.update(params)\n",
    "    macro_F1_metric=[]\n",
    "    for model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n",
    "        # y_test_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_test_pred=model.predict(np.array(X)[test_index]).argmax(axis=1)\n",
    "        y_test=np.array(Y)[test_index]\n",
    "\n",
    "        score = accuracy_score(y_test, y_test_pred)\n",
    "        print(\"Accuracy score is \" + str(score))\n",
    "        report=classification_report(y_test, y_test_pred)\n",
    "        macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n",
    "        with open('xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n",
    "            pickle.dump(model, f)#store as pickle\n",
    "        print('model saved to xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]))\n",
    "        print(report)\n",
    "\n",
    "        # Build the plot\n",
    "#         cm = confusion_matrix(y_test, y_test_pred)\n",
    "#         plt.figure(figsize=(16,7))\n",
    "#         sb.set(font_scale=1.4)\n",
    "#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n",
    "\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title('Confusion Matrix for Random Forest Model')\n",
    "#         plt.show()\n",
    "    wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\n",
    "    print(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric),depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d394104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params, train_data , 700, evals=[(train_data,'eval'),(validation_data, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30b3ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"5bc70e88e5aac2b0cd097233114e75b47f6888e6\")\n",
    "wandb.init(project=\"test-project\", entity=\"cz4041\",id='xgb_depth_test')\n",
    "for depth in range(2,30):\n",
    "    params={ 'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':depth, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2}\n",
    "    wandb.config.update(params)\n",
    "    macro_F1_metric=[]\n",
    "    for model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n",
    "        # y_test_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_test_pred=model.predict(np.array(X)[test_index]).argmax(axis=1)\n",
    "        y_test=np.array(Y)[test_index]\n",
    "\n",
    "        score = accuracy_score(y_test, y_test_pred)\n",
    "        print(\"Accuracy score is \" + str(score))\n",
    "        report=classification_report(y_test, y_test_pred)\n",
    "        macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n",
    "        with open('xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n",
    "            pickle.dump(model, f)#store as pickle\n",
    "        print('model saved to xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]))\n",
    "        print(report)\n",
    "\n",
    "        # Build the plot\n",
    "#         cm = confusion_matrix(y_test, y_test_pred)\n",
    "#         plt.figure(figsize=(16,7))\n",
    "#         sb.set(font_scale=1.4)\n",
    "#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n",
    "\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title('Confusion Matrix for Random Forest Model')\n",
    "#         plt.show()\n",
    "    wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\n",
    "    print(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric),depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6325a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params, train_data , 200, evals=[(train_data,'eval'),(validation_data, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c9e2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X_train,y_train,X_test,y_test,params=None):\n",
    "    train_data = xgb.DMatrix(X_train, y_train)\n",
    "    validation_data = xgb.DMatrix(X_test, y_test)\n",
    "    if params is None:\n",
    "        params = {\n",
    "'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':7, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2\n",
    "}\n",
    "    clf = xgb.train(params, train_data , 200, evals=[(train_data,'eval'),(validation_data, 'eval')], verbose_eval=True)\n",
    "    return clf\n",
    "\n",
    "def k_fold(k,X,Y,shuffle=False,random_state=None,params=None):\n",
    "    X,Y=np.array(X),np.array(Y)\n",
    "    kf = KFold(n_splits=k,random_state=random_state, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = Y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = Y[test_index]\n",
    "        model = xgboost_train(X_train,y_train,X_test,y_test,params)\n",
    "        yield (model,test_index)\n",
    "# X = data_df.iloc[:, :-1]\n",
    "# Y = data_df[\"label\"]\n",
    "# k_fold(3,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4a37026",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df.iloc[:, :-1]\n",
    "Y = data_df[\"label\"]\n",
    "print(\"original distribution:\",Counter(Y))\n",
    "#upsamlping \n",
    "smo = SMOTE(random_state=42)\n",
    "X_smo, Y_smo = smo.fit_resample(X, Y)\n",
    "print(\"smote distribution:\",Counter(Y_smo))\n",
    "X,Y=X_smo, Y_smo#using smote dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8baf0913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xgb_depth_test) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1822... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">xgb_depth_test</strong>: <a href=\"https://wandb.ai/cz4041/test-project/runs/xgb_depth_test\" target=\"_blank\">https://wandb.ai/cz4041/test-project/runs/xgb_depth_test</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220202_031434-xgb_depth_test/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xgb_depth_test). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/cz4041/test-project/runs/xgb_depth_test\" target=\"_blank\">xgb_depth_test</a></strong> to <a href=\"https://wandb.ai/cz4041/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key=\"5bc70e88e5aac2b0cd097233114e75b47f6888e6\")\n",
    "wandb.init(project=\"test-project\", entity=\"cz4041\",id='xgb_depth_test')\n",
    "for depth in range(2,30):\n",
    "    params={ 'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':depth, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2}\n",
    "    wandb.config.update(params)\n",
    "    macro_F1_metric=[]\n",
    "    for model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n",
    "        # y_test_pred = model.predict(X_test).argmax(axis=1)\n",
    "        y_test_pred=model.predict(np.array(X)[test_index]).argmax(axis=1)\n",
    "        y_test=np.array(Y)[test_index]\n",
    "\n",
    "        score = accuracy_score(y_test, y_test_pred)\n",
    "        print(\"Accuracy score is \" + str(score))\n",
    "        report=classification_report(y_test, y_test_pred)\n",
    "        macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n",
    "        with open('xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n",
    "            pickle.dump(model, f)#store as pickle\n",
    "        print('model saved to xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]))\n",
    "        print(report)\n",
    "\n",
    "        # Build the plot\n",
    "#         cm = confusion_matrix(y_test, y_test_pred)\n",
    "#         plt.figure(figsize=(16,7))\n",
    "#         sb.set(font_scale=1.4)\n",
    "#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n",
    "\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title('Confusion Matrix for Random Forest Model')\n",
    "#         plt.show()\n",
    "    wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\n",
    "    print(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric),depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4abaea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"5bc70e88e5aac2b0cd097233114e75b47f6888e6\")\n",
    "wandb.init(project=\"test-project\", entity=\"cz4041\",id='xgb_depth_test')\n",
    "for depth in range(2,30):\n",
    "    params={ 'boosting_type': 'dart',\n",
    "'objective':'multi:softmax',\n",
    "'num_class':9,\n",
    "'max_depth':depth, #Max_depth: The maximum depth of a tree.\n",
    "'min_child_weight':20, #Min_child_weight: The minimum sum of instance weight needed in a child. Keeping it high prevents the child from being too specific and thus helps to avoid overfitting.\n",
    "'gamma':1,#Minimum loss reduction required to make a further partition on a leaf node of the tree. Again, the larger the game, the less likely the model will overfit.\n",
    "'subsample':0.8,#The ratio of rows that are randomly selected prior to growing trees. Subsample can also be used to avoid overfitting\n",
    "'colsample_bytree':0.7,\n",
    "'tree_method':'hist',\n",
    "'eval_metric':'mlogloss',\n",
    "'eta':0.04,#The learning rate. Keeping it high makes the model learn fast but increases the chance of overfitting at the same time\n",
    "'alpha': 1,#L1 regularization term\n",
    "'verbose': 2}\n",
    "    wandb.config.update(params)\n",
    "    macro_F1_metric=[]\n",
    "    for model,test_index in k_fold(5,X,Y,shuffle=True,random_state=1,params=params):#k_fold(k,X,Y,shuffle=False,random_state=None)\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, Y,shuffle = \"True\", test_size =0.999)\n",
    "        # y_test_pred = model.predict(X_test).argmax(axis=1)\n",
    "        X_test = np.array(X)[test_index]\n",
    "        y_test = np.array(Y)[test_index]\n",
    "        X_test_Dmax = xgb.DMatrix(X_test)\n",
    "        y_test_pred=model.predict(X_test_Dmax)\n",
    "\n",
    "        score = accuracy_score(y_test, y_test_pred)\n",
    "        print(\"Accuracy score is \" + str(score))\n",
    "        report=classification_report(y_test, y_test_pred)\n",
    "        macro_F1_metric.append(eval(report.split(\"\\n\")[8].split()[4]))\n",
    "        with open('xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]), 'wb') as f:\n",
    "            pickle.dump(model, f)#store as pickle\n",
    "        print('model saved to xgb_data/xgb_model_{}.pkl'.format(macro_F1_metric[-1]))\n",
    "        print(report)\n",
    "\n",
    "        # Build the plot\n",
    "#         cm = confusion_matrix(y_test, y_test_pred)\n",
    "#         plt.figure(figsize=(16,7))\n",
    "#         sb.set(font_scale=1.4)\n",
    "#         sb.heatmap(cm, annot=True, annot_kws={'size':18},cmap=plt.cm.Greens, fmt=\".0f\")\n",
    "\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title('Confusion Matrix for Random Forest Model')\n",
    "#         plt.show()\n",
    "    wandb.log({\"avg macro_F1_metric\": sum(macro_F1_metric)/len(macro_F1_metric)})\n",
    "    print(\"avg macro_F1_metric:\",sum(macro_F1_metric)/len(macro_F1_metric),depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
